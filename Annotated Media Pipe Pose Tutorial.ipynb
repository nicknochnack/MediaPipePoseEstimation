{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50e3013c",
   "metadata": {},
   "source": [
    "1. Installing libraries\n",
    "\n",
    "This line installs the required packages for the tutorial, specifically `mediapipe` (which is a library for hand, pose, and face detection) and `opencv-python` (a library for computer vision tasks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a84f9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in /Users/vijaishankarbhavanishankar/anaconda3/lib/python3.11/site-packages (0.10.7)\n",
      "Requirement already satisfied: opencv-python in /Users/vijaishankarbhavanishankar/anaconda3/lib/python3.11/site-packages (4.8.1.78)\n",
      "Requirement already satisfied: absl-py in /Users/vijaishankarbhavanishankar/anaconda3/lib/python3.11/site-packages (from mediapipe) (2.0.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /Users/vijaishankarbhavanishankar/anaconda3/lib/python3.11/site-packages (from mediapipe) (22.1.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /Users/vijaishankarbhavanishankar/anaconda3/lib/python3.11/site-packages (from mediapipe) (23.5.26)\n",
      "Requirement already satisfied: matplotlib in /Users/vijaishankarbhavanishankar/anaconda3/lib/python3.11/site-packages (from mediapipe) (3.7.2)\n",
      "Requirement already satisfied: numpy in /Users/vijaishankarbhavanishankar/anaconda3/lib/python3.11/site-packages (from mediapipe) (1.24.3)\n",
      "Requirement already satisfied: opencv-contrib-python in /Users/vijaishankarbhavanishankar/anaconda3/lib/python3.11/site-packages (from mediapipe) (4.8.1.78)\n",
      "Requirement already satisfied: protobuf<4,>=3.11 in /Users/vijaishankarbhavanishankar/anaconda3/lib/python3.11/site-packages (from mediapipe) (3.20.3)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in /Users/vijaishankarbhavanishankar/anaconda3/lib/python3.11/site-packages (from mediapipe) (0.4.6)\n",
      "Requirement already satisfied: CFFI>=1.0 in /Users/vijaishankarbhavanishankar/anaconda3/lib/python3.11/site-packages (from sounddevice>=0.4.4->mediapipe) (1.15.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/vijaishankarbhavanishankar/anaconda3/lib/python3.11/site-packages (from matplotlib->mediapipe) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/vijaishankarbhavanishankar/anaconda3/lib/python3.11/site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/vijaishankarbhavanishankar/anaconda3/lib/python3.11/site-packages (from matplotlib->mediapipe) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/vijaishankarbhavanishankar/anaconda3/lib/python3.11/site-packages (from matplotlib->mediapipe) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/vijaishankarbhavanishankar/anaconda3/lib/python3.11/site-packages (from matplotlib->mediapipe) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/vijaishankarbhavanishankar/anaconda3/lib/python3.11/site-packages (from matplotlib->mediapipe) (9.4.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /Users/vijaishankarbhavanishankar/anaconda3/lib/python3.11/site-packages (from matplotlib->mediapipe) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/vijaishankarbhavanishankar/anaconda3/lib/python3.11/site-packages (from matplotlib->mediapipe) (2.8.2)\n",
      "Requirement already satisfied: pycparser in /Users/vijaishankarbhavanishankar/anaconda3/lib/python3.11/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in /Users/vijaishankarbhavanishankar/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install mediapipe opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f0884c",
   "metadata": {},
   "source": [
    "2. Installing Dependencies \n",
    "\n",
    "Here, the necessary libraries are imported:\n",
    "\n",
    "- `cv2`: This is the OpenCV library which is used for various computer vision tasks.\n",
    "- `mediapipe as mp`: Mediapipe is imported with the alias `mp`.\n",
    "- `numpy as np`: Numpy is imported with the alias `np`.\n",
    "- `mp_drawing`: This is a utility from Mediapipe to help with drawing on images.\n",
    "- `mp_pose`: This helps in detecting human poses using Mediapipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3ba69a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06df7edb",
   "metadata": {},
   "source": [
    "3. This block of code sets up a video feed using OpenCV.\n",
    "\n",
    "- `cv2.VideoCapture(0)`: This initializes the video capture with the primary webcam of the computer.\n",
    "- `while cap.isOpened()`: Continuously capture frames as long as the webcam is active.\n",
    "- `ret, frame = cap.read()`: Read a frame from the webcam. `ret` is a boolean that indicates if the frame was successfully grabbed.\n",
    "- `cv2.imshow('Mediapipe Feed', frame)`: This displays the captured frame in a window named 'Mediapipe Feed'.\n",
    "- `if cv2.waitKey(10) & 0xFF == ord('q')`: This checks if the 'q' key is pressed. If pressed, it breaks out of the loop to stop capturing.\n",
    "- `cap.release()`: Releases the video capture object.\n",
    "- `cv2.destroyAllWindows()`: Closes all OpenCV windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd376e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIDEO FEED\n",
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    cv2.imshow('Mediapipe Feed', frame)\n",
    "    \n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a277cf",
   "metadata": {},
   "source": [
    "4. Pose detection\n",
    "\n",
    "This block of code captures video from the webcam and processes each frame using Mediapipe to detect human poses.\n",
    "The code inside the `with` statement initializes the pose detection from Mediapipe.\n",
    "Each frame from the webcam is converted from BGR (Blue, Green, Red - the default format of OpenCV) to RGB (Red, Green, Blue - the format Mediapipe uses).\n",
    "`pose.process(image)`: Detects the pose in the image.\n",
    "The detected landmarks (keypoints of the pose) are then drawn on the image using `mp_drawing.draw_landmarks(...)`. The processed image is then displayed using `cv2.imshow(...)`. The loop continues until the 'q' key is pressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4798595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "## Setup mediapipe instance\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # Recolor image to RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "      \n",
    "        # Make detection\n",
    "        results = pose.process(image)\n",
    "    \n",
    "        # Recolor back to BGR\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Render detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2), \n",
    "                                mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2) \n",
    "                                 )               \n",
    "        \n",
    "        cv2.imshow('Mediapipe Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b11bb91",
   "metadata": {},
   "source": [
    "This line is likely an interactive command to inspect the documentation or source code of the `DrawingSpec` class from Mediapipe. In Jupyter notebooks, appending `??` to a function or class name displays its source code and docstring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a04fc6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mmp_drawing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDrawingSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcolor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mthickness\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcircle_radius\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m      DrawingSpec(color: Tuple[int, int, int] = (224, 224, 224), thickness: int = 2, circle_radius: int = 2)\n",
      "\u001b[0;31mSource:\u001b[0m        \n",
      "\u001b[0;34m@\u001b[0m\u001b[0mdataclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataclass\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0mDrawingSpec\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;31m# Color for drawing the annotation. Default to the white color.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0mcolor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWHITE_COLOR\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;31m# Thickness for drawing the annotation. Default to 2 pixels.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0mthickness\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;31m# Circle radius. Default to 2 pixels.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0mcircle_radius\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m           ~/anaconda3/lib/python3.11/site-packages/mediapipe/python/solutions/drawing_utils.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "mp_drawing.DrawingSpec??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5899c6a",
   "metadata": {},
   "source": [
    "This block of code is very similar to the one we discussed earlier. The main addition is the extraction of landmarks.\n",
    "The line `landmarks = results.pose_landmarks.landmark` extracts the pose landmarks (keypoints) detected by Mediapipe. These landmarks represent different parts of the body, like the nose, eyes, shoulders, elbows, etc.\n",
    "The extracted landmarks are then printed to the console.\n",
    "The `try` and `except` block is used to handle any potential errors that might arise when extracting landmarks. If there's an error (e.g., no landmarks detected), the code inside the `except` block is executed, which in this case, does nothing (using `pass`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e396dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "## Setup mediapipe instance\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # Recolor image to RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "      \n",
    "        # Make detection\n",
    "        results = pose.process(image)\n",
    "    \n",
    "        # Recolor back to BGR\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Extract landmarks\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            print(landmarks)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        # Render detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2), \n",
    "                                mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2) \n",
    "                                 )               \n",
    "        \n",
    "        cv2.imshow('Mediapipe Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7188ab8b",
   "metadata": {},
   "source": [
    "This line simply gets and displays the number of landmarks detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563b7fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(landmarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b77f08d",
   "metadata": {},
   "source": [
    "This block of code prints out all the pose landmarks that Mediapipe can detect. `mp_pose.PoseLandmark` is an enumeration of all possible pose landmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025e7cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lndmrk in mp_pose.PoseLandmark:\n",
    "    print(lndmrk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6865dd9e",
   "metadata": {},
   "source": [
    "This line gets the visibility score of the left shoulder landmark. The visibility score indicates the probability of the landmark being visible in the image (not occluded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0b6d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].visibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cfa281",
   "metadata": {},
   "source": [
    "This line retrieves details about the left elbow landmark, such as its coordinates and visibility score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8fb229",
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a617e626",
   "metadata": {},
   "source": [
    "This line retrieves the details of the left wrist landmark, such as its coordinates (x, y, and z) and potentially other details like visibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8582414",
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19970f40",
   "metadata": {},
   "source": [
    "\n",
    "This function, named `calculate_angle`, is designed to compute the angle formed by three points: `a`, `b`, and `c`, where `b` acts as the vertex of the angle.\n",
    "The function performs the following steps:\n",
    "- Converts the points `a`, `b`, and `c` to numpy arrays to facilitate mathematical operations.\n",
    "- Calculates the angle using the arctangent function. The difference in the arctangents of the slopes defined by the points gives the angle in radians.\n",
    "- Converts the angle from radians to degrees.\n",
    "- If the resulting angle exceeds 180 degrees, it calculates the supplementary angle.\n",
    "- Returns the computed angle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc05493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angle(a,b,c):\n",
    "    a = np.array(a) # First\n",
    "    b = np.array(b) # Mid\n",
    "    c = np.array(c) # End\n",
    "    \n",
    "    radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])\n",
    "    angle = np.abs(radians*180.0/np.pi)\n",
    "    \n",
    "    if angle >180.0:\n",
    "        angle = 360-angle\n",
    "        \n",
    "    return angle "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d24750",
   "metadata": {},
   "source": [
    "\n",
    "This block captures video from the webcam and processes each frame with Mediapipe to detect human poses.\n",
    "The major steps involved are:\n",
    "1. **Frame Capture**: The video capture is initialized, and frames are continuously read.\n",
    "2. **Color Conversion**: Each frame is converted from BGR to RGB, which is the format expected by Mediapipe.\n",
    "3. **Pose Detection**: The frame is processed using Mediapipe's pose detection. The result contains pose landmarks.\n",
    "4. **Angle Calculation**:\n",
    "   - The landmarks corresponding to the left shoulder, elbow, and wrist are extracted.\n",
    "   - The angle at the elbow (formed by the shoulder, elbow, and wrist) is calculated using the `calculate_angle` function.\n",
    "   - This angle is displayed on the video feed near the elbow position.\n",
    "5. **Bicep Curl Detection**: A simple logic is implemented to detect the motion of a bicep curl exercise:\n",
    "   - When the arm is fully extended (angle > 160 degrees), the stage is set to \"down\".\n",
    "   - If the arm is curled (angle < 30 degrees) and the previous stage was \"down\", the counter increments, indicating a completed bicep curl repetition.\n",
    "6. **Display Information**: The counter and the stage (\"up\" or \"down\") are displayed on the video feed.\n",
    "7. **Landmark Rendering**: Detected landmarks and their connections are drawn on the frame.\n",
    "8. **Display Video Feed**: The processed frame, with landmarks and angle information, is displayed in a window.\n",
    "9. **Exit Condition**: The video feed can be stopped by pressing the 'q' key.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e7ff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "## Setup mediapipe instance\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # Recolor image to RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "      \n",
    "        # Make detection\n",
    "        results = pose.process(image)\n",
    "    \n",
    "        # Recolor back to BGR\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Extract landmarks\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            \n",
    "            # Get coordinates\n",
    "            shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x,landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]\n",
    "            elbow = [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x,landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]\n",
    "            wrist = [landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].x,landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].y]\n",
    "            \n",
    "            # Calculate angle\n",
    "            angle = calculate_angle(shoulder, elbow, wrist)\n",
    "            \n",
    "            # Visualize angle\n",
    "            cv2.putText(image, str(angle), \n",
    "                           tuple(np.multiply(elbow, [640, 480]).astype(int)), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA\n",
    "                                )\n",
    "                       \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        # Render detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2), \n",
    "                                mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2) \n",
    "                                 )               \n",
    "        \n",
    "        cv2.imshow('Mediapipe Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb98993e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Curl counter variables\n",
    "counter = 0 \n",
    "stage = None\n",
    "\n",
    "## Setup mediapipe instance\n",
    "with mp_pose.Pose(min_detection_confidence=0.1, min_tracking_confidence=0.1) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # Recolor image to RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "      \n",
    "        # Make detection\n",
    "        results = pose.process(image)\n",
    "    \n",
    "        # Recolor back to BGR\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Extract landmarks\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            \n",
    "            # Get coordinates\n",
    "            shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x,landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]\n",
    "            elbow = [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x,landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]\n",
    "            wrist = [landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].x,landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].y]\n",
    "            \n",
    "            # Calculate angle\n",
    "            angle = calculate_angle(shoulder, elbow, wrist)\n",
    "            \n",
    "            # Visualize angle\n",
    "            cv2.putText(image, str(angle), \n",
    "                           tuple(np.multiply(elbow, [640, 480]).astype(int)), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA\n",
    "                                )\n",
    "            \n",
    "            # Curl counter logic\n",
    "            if angle > 160:\n",
    "                stage = \"down\"\n",
    "            if angle < 30 and stage =='down':\n",
    "                stage=\"up\"\n",
    "                counter +=1\n",
    "                print(counter)\n",
    "                       \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Render curl counter\n",
    "        # Setup status box\n",
    "        cv2.rectangle(image, (0,0), (225,73), (245,117,16), -1)\n",
    "        \n",
    "        # Rep data\n",
    "        cv2.putText(image, 'REPS', (15,12), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 1, cv2.LINE_AA)\n",
    "        cv2.putText(image, str(counter), \n",
    "                    (10,60), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 2, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Stage data\n",
    "        cv2.putText(image, 'STAGE', (65,12), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 1, cv2.LINE_AA)\n",
    "        cv2.putText(image, stage, \n",
    "                    (60,60), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 2, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        \n",
    "        # Render detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2), \n",
    "                                mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2) \n",
    "                                 )               \n",
    "        \n",
    "        cv2.imshow('Mediapipe Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
