{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50e3013c",
   "metadata": {},
   "source": [
    "1. Installing libraries\n",
    "\n",
    "This line installs the required packages for the tutorial, specifically `mediapipe` (which is a library for hand, pose, and face detection) and `opencv-python` (a library for computer vision tasks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a84f9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mediapipe opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f0884c",
   "metadata": {},
   "source": [
    "2. Installing Dependencies \n",
    "\n",
    "Here, the necessary libraries are imported:\n",
    "\n",
    "- `cv2`: This is the OpenCV library which is used for various computer vision tasks.\n",
    "- `mediapipe as mp`: Mediapipe is imported with the alias `mp`.\n",
    "- `numpy as np`: Numpy is imported with the alias `np`.\n",
    "- `mp_drawing`: This is a utility from Mediapipe to help with drawing on images.\n",
    "- `mp_pose`: This helps in detecting human poses using Mediapipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ba69a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06df7edb",
   "metadata": {},
   "source": [
    "3. This block of code sets up a video feed using OpenCV.\n",
    "\n",
    "- `cv2.VideoCapture(0)`: This initializes the video capture with the primary webcam of the computer.\n",
    "- `while cap.isOpened()`: Continuously capture frames as long as the webcam is active.\n",
    "- `ret, frame = cap.read()`: Read a frame from the webcam. `ret` is a boolean that indicates if the frame was successfully grabbed.\n",
    "- `cv2.imshow('Mediapipe Feed', frame)`: This displays the captured frame in a window named 'Mediapipe Feed'.\n",
    "- `if cv2.waitKey(10) & 0xFF == ord('q')`: This checks if the 'q' key is pressed. If pressed, it breaks out of the loop to stop capturing.\n",
    "- `cap.release()`: Releases the video capture object.\n",
    "- `cv2.destroyAllWindows()`: Closes all OpenCV windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcf1691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIDEO FEED\n",
    "cap = cv2.VideoCapture(1)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab a frame\")\n",
    "        break\n",
    "    cv2.imshow('Mediapipe Feed', frame)\n",
    "\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a277cf",
   "metadata": {},
   "source": [
    "4. Pose detection\n",
    "\n",
    "This block of code captures video from the webcam and processes each frame using Mediapipe to detect human poses.\n",
    "The code inside the `with` statement initializes the pose detection from Mediapipe.\n",
    "Each frame from the webcam is converted from BGR (Blue, Green, Red - the default format of OpenCV) to RGB (Red, Green, Blue - the format Mediapipe uses).\n",
    "`pose.process(image)`: Detects the pose in the image.\n",
    "The detected landmarks (keypoints of the pose) are then drawn on the image using `mp_drawing.draw_landmarks(...)`. The processed image is then displayed using `cv2.imshow(...)`. The loop continues until the 'q' key is pressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4798595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "## Setup mediapipe instance\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # Recolor image to RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "      \n",
    "        # Make detection\n",
    "        results = pose.process(image)\n",
    "    \n",
    "        # Recolor back to BGR\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Render detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2), \n",
    "                                mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2) \n",
    "                                 )               \n",
    "        \n",
    "        cv2.imshow('Mediapipe Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b11bb91",
   "metadata": {},
   "source": [
    "This line is likely an interactive command to inspect the documentation or source code of the `DrawingSpec` class from Mediapipe. In Jupyter notebooks, appending `??` to a function or class name displays its source code and docstring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04fc6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing.DrawingSpec??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5899c6a",
   "metadata": {},
   "source": [
    "This block of code is very similar to the one we discussed earlier. The main addition is the extraction of landmarks.\n",
    "The line `landmarks = results.pose_landmarks.landmark` extracts the pose landmarks (keypoints) detected by Mediapipe. These landmarks represent different parts of the body, like the nose, eyes, shoulders, elbows, etc.\n",
    "The extracted landmarks are then printed to the console.\n",
    "The `try` and `except` block is used to handle any potential errors that might arise when extracting landmarks. If there's an error (e.g., no landmarks detected), the code inside the `except` block is executed, which in this case, does nothing (using `pass`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e396dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "## Setup mediapipe instance\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # Recolor image to RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "      \n",
    "        # Make detection\n",
    "        results = pose.process(image)\n",
    "    \n",
    "        # Recolor back to BGR\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Extract landmarks\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            print(landmarks)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        # Render detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2), \n",
    "                                mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2) \n",
    "                                 )               \n",
    "        \n",
    "        cv2.imshow('Mediapipe Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7188ab8b",
   "metadata": {},
   "source": [
    "This line simply gets and displays the number of landmarks detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563b7fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(landmarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b77f08d",
   "metadata": {},
   "source": [
    "This block of code prints out all the pose landmarks that Mediapipe can detect. `mp_pose.PoseLandmark` is an enumeration of all possible pose landmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025e7cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lndmrk in mp_pose.PoseLandmark:\n",
    "    print(lndmrk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6865dd9e",
   "metadata": {},
   "source": [
    "This line gets the visibility score of the left shoulder landmark. The visibility score indicates the probability of the landmark being visible in the image (not occluded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0b6d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].visibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cfa281",
   "metadata": {},
   "source": [
    "This line retrieves details about the left elbow landmark, such as its coordinates and visibility score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8fb229",
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a617e626",
   "metadata": {},
   "source": [
    "This line retrieves the details of the left wrist landmark, such as its coordinates (x, y, and z) and potentially other details like visibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8582414",
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19970f40",
   "metadata": {},
   "source": [
    "\n",
    "This function, named `calculate_angle`, is designed to compute the angle formed by three points: `a`, `b`, and `c`, where `b` acts as the vertex of the angle.\n",
    "The function performs the following steps:\n",
    "- Converts the points `a`, `b`, and `c` to numpy arrays to facilitate mathematical operations.\n",
    "- Calculates the angle using the arctangent function. The difference in the arctangents of the slopes defined by the points gives the angle in radians.\n",
    "- Converts the angle from radians to degrees.\n",
    "- If the resulting angle exceeds 180 degrees, it calculates the supplementary angle.\n",
    "- Returns the computed angle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc05493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angle(a,b,c):\n",
    "    a = np.array(a) # First\n",
    "    b = np.array(b) # Mid\n",
    "    c = np.array(c) # End\n",
    "    \n",
    "    radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])\n",
    "    angle = np.abs(radians*180.0/np.pi)\n",
    "    \n",
    "    if angle >180.0:\n",
    "        angle = 360-angle\n",
    "        \n",
    "    return angle "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d24750",
   "metadata": {},
   "source": [
    "\n",
    "This block captures video from the webcam and processes each frame with Mediapipe to detect human poses.\n",
    "The major steps involved are:\n",
    "1. **Frame Capture**: The video capture is initialized, and frames are continuously read.\n",
    "2. **Color Conversion**: Each frame is converted from BGR to RGB, which is the format expected by Mediapipe.\n",
    "3. **Pose Detection**: The frame is processed using Mediapipe's pose detection. The result contains pose landmarks.\n",
    "4. **Angle Calculation**:\n",
    "   - The landmarks corresponding to the left shoulder, elbow, and wrist are extracted.\n",
    "   - The angle at the elbow (formed by the shoulder, elbow, and wrist) is calculated using the `calculate_angle` function.\n",
    "   - This angle is displayed on the video feed near the elbow position.\n",
    "5. **Bicep Curl Detection**: A simple logic is implemented to detect the motion of a bicep curl exercise:\n",
    "   - When the arm is fully extended (angle > 160 degrees), the stage is set to \"down\".\n",
    "   - If the arm is curled (angle < 30 degrees) and the previous stage was \"down\", the counter increments, indicating a completed bicep curl repetition.\n",
    "6. **Display Information**: The counter and the stage (\"up\" or \"down\") are displayed on the video feed.\n",
    "7. **Landmark Rendering**: Detected landmarks and their connections are drawn on the frame.\n",
    "8. **Display Video Feed**: The processed frame, with landmarks and angle information, is displayed in a window.\n",
    "9. **Exit Condition**: The video feed can be stopped by pressing the 'q' key.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e7ff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(1)\n",
    "## Setup mediapipe instance\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # Recolor image to RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "      \n",
    "        # Make detection\n",
    "        results = pose.process(image)\n",
    "    \n",
    "        # Recolor back to BGR\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Extract landmarks\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            \n",
    "            # Get coordinates\n",
    "            shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x,landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]\n",
    "            elbow = [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x,landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]\n",
    "            wrist = [landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].x,landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].y]\n",
    "            \n",
    "            # Calculate angle\n",
    "            angle = calculate_angle(shoulder, elbow, wrist)\n",
    "            \n",
    "            # Visualize angle\n",
    "            cv2.putText(image, str(angle), \n",
    "                           tuple(np.multiply(elbow, [640, 480]).astype(int)), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA\n",
    "                                )\n",
    "                       \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        # Render detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2), \n",
    "                                mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2) \n",
    "                                 )               \n",
    "        \n",
    "        cv2.imshow('Mediapipe Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb98993e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "# Curl counter variables\n",
    "counter = 0 \n",
    "stage = None\n",
    "\n",
    "## Setup mediapipe instance\n",
    "with mp_pose.Pose(min_detection_confidence=0.1, min_tracking_confidence=0.1) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # Recolor image to RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "      \n",
    "        # Make detection\n",
    "        results = pose.process(image)\n",
    "    \n",
    "        # Recolor back to BGR\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Extract landmarks\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            \n",
    "            # Get coordinates\n",
    "            shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x,landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]\n",
    "            elbow = [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x,landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]\n",
    "            wrist = [landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].x,landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].y]\n",
    "            \n",
    "            # Calculate angle\n",
    "            angle = calculate_angle(shoulder, elbow, wrist)\n",
    "            \n",
    "            # Visualize angle\n",
    "            cv2.putText(image, str(angle), \n",
    "                           tuple(np.multiply(elbow, [640, 480]).astype(int)), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA\n",
    "                                )\n",
    "            \n",
    "            # Curl counter logic\n",
    "            if angle > 160:\n",
    "                stage = \"down\"\n",
    "            if angle < 30 and stage =='down':\n",
    "                stage=\"up\"\n",
    "                counter +=1\n",
    "                print(counter)\n",
    "                       \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Render curl counter\n",
    "        # Setup status box\n",
    "        cv2.rectangle(image, (0,0), (225,73), (245,117,16), -1)\n",
    "        \n",
    "        # Rep data\n",
    "        cv2.putText(image, 'REPS', (15,12), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 1, cv2.LINE_AA)\n",
    "        cv2.putText(image, str(counter), \n",
    "                    (10,60), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 2, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Stage data\n",
    "        cv2.putText(image, 'STAGE', (65,12), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 1, cv2.LINE_AA)\n",
    "        cv2.putText(image, stage, \n",
    "                    (60,60), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 2, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        \n",
    "        # Render detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2), \n",
    "                                mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2) \n",
    "                                 )               \n",
    "        \n",
    "        cv2.imshow('Mediapipe Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db12dfe",
   "metadata": {},
   "source": [
    "tasks for squat tutorial\n",
    "\n",
    "1. leg and hand positions before start of exercise\n",
    "2. signal that the exercise person ready\n",
    "3. countdown to exercise \n",
    "4. counting reps\n",
    "5. countingn time - up and down\n",
    "6. create a mock up of a gym camera so we have the same reference points \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "928524af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x44495658/'XVID' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.233333333333333\n",
      "6.133333333333334\n",
      "8.0\n",
      "10.0\n",
      "18.133333333333333\n",
      "20.033333333333335\n",
      "22.0\n",
      "23.933333333333334\n",
      "25.9\n",
      "Failed to grab a frame\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# Define a function that calculates an angle between three points in x,y space\n",
    "def calculate_angle(a,b,c):\n",
    "    a = np.array(a) # First\n",
    "    b = np.array(b) # Mid\n",
    "    c = np.array(c) # End\n",
    "    \n",
    "    radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])\n",
    "    angle = np.abs(radians*180.0/np.pi)\n",
    "    \n",
    "    if angle >180.0:\n",
    "        angle = 360-angle\n",
    "    return angle \n",
    "\n",
    "# Specify the location of the file with the video to be read\n",
    "filename_r = '/Users/vijaishankarbhavanishankar/Documents/EschAR/MediaPipePoseEstimation-eschar/Data/videos/IMG_3620_RepCountV2.mp4'\n",
    "# Specify the location where the new video with detections will be written\n",
    "filename_w = '/Users/vijaishankarbhavanishankar/Documents/EschAR/MediaPipePoseEstimation-eschar/Data/videos/IMG_3620_RepCountV2-output.mp4'\n",
    "# Capture the video from the file\n",
    "cap = cv2.VideoCapture(filename_r)\n",
    "\n",
    "# Get video frame dimensions and fps\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "frame_size = (frame_width,frame_height)\n",
    "fps = cap.get(5)\n",
    "\n",
    "# Create a video writer object\n",
    "output = cv2.VideoWriter(filename_w, cv2.VideoWriter_fourcc(*'XVID'), fps, frame_size)\n",
    "\n",
    "# Initialize the Rep Counter variables\n",
    "real_counter = 0\n",
    "stage = None\n",
    "rep_count = 0\n",
    "last_event_time = None\n",
    "frame_count = 0\n",
    "\n",
    "## Setup mediapipe instance\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "\n",
    "    fps = cap.get(5)\n",
    "    frame_width  = int(cap.get(3))\n",
    "    frame_height = int(cap.get(4))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to grab a frame\")\n",
    "            break\n",
    "        # Count frames and calculate time in video\n",
    "        frame_count += 1\n",
    "        time = frame_count / fps #[s]\n",
    "\n",
    "        # Recolor image to RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "      \n",
    "        # Make detection\n",
    "        results = pose.process(image)\n",
    "    \n",
    "        # Recolor back to BGR\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Extract landmarks\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            \n",
    "            # Get coordinates\n",
    "            hip = [landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].x,landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].y]\n",
    "            knee = [landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].x,landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].y]\n",
    "            ankle = [landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].x,landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].y]\n",
    "            \n",
    "            # Calculate angle\n",
    "            angle = calculate_angle(hip, knee, ankle)\n",
    "            \n",
    "            # Visualize angle\n",
    "            cv2.putText(image, f\"{angle:.1f}\", \n",
    "                           tuple(np.multiply(knee, frame_size).astype(int)), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA\n",
    "                                )\n",
    "            \n",
    "            ##########################################################################################################################\n",
    "            # Rep Counter logic\n",
    "            # The rep is registered when the knee bend angle goes from the upper limit to the lower limit\n",
    "            upper_limit = 160\n",
    "            lower_limit = 80\n",
    "            # Specify the minimum rep number to start registering the set\n",
    "            min_rep_count = 2\n",
    "            # Specify the minimum time between reps\n",
    "            min_rep_time = 3 #[s]\n",
    "            # Above the upper angle limit, register the \"hold up\" stage\n",
    "            if angle >= upper_limit:\n",
    "                stage = 'hold up'\n",
    "            # Below the upper limit and after the \"hold up\" stage, register \"down\" stage\n",
    "            if angle < upper_limit and stage == 'hold up':\n",
    "                stage = 'down'\n",
    "            # Below the lower limit and after the \"down\" stage, register the \"hold down\" stage\n",
    "            if angle < lower_limit and stage =='down':\n",
    "                stage = 'hold down'\n",
    "            # Above the lower limit and after the \"hold down\" stage, register the \"up\" stage and count the rep\n",
    "            if angle > lower_limit and stage == 'hold down':\n",
    "                stage = 'up'\n",
    "                rep_count +=1\n",
    "                last_event_time = time\n",
    "                print(last_event_time)\n",
    "\n",
    "            # Register the real rep count only for sets with more than the min rep count\n",
    "            if rep_count >= min_rep_count:\n",
    "                real_counter = rep_count\n",
    "            # Restart the counter when the reps do not repeat in less than the min rep time\n",
    "            if time - last_event_time > min_rep_time:\n",
    "                rep_count = 0\n",
    "            ##########################################################################################################################\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        # Render Rep Counter\n",
    "        # Set the position of the Rep Counter window\n",
    "        x_offset = 50\n",
    "        y_offset = 100\n",
    "        # Setup status box\n",
    "        cv2.rectangle(image, (x_offset,y_offset), (390+x_offset,73+y_offset), (128,128,128), -1)\n",
    "        # Rep data\n",
    "        cv2.putText(image, 'REPS', (15+x_offset,12+y_offset), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 1, cv2.LINE_AA)\n",
    "        cv2.putText(image, str(real_counter), \n",
    "                    (10+x_offset,60+y_offset), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 2, (255,255,255), 2, cv2.LINE_AA)\n",
    "        # Stage data\n",
    "        cv2.putText(image, 'STAGE', (65+x_offset,12+y_offset), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 1, cv2.LINE_AA)\n",
    "        cv2.putText(image, stage, \n",
    "                    (60+x_offset,60+y_offset), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 2, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "\n",
    "        # Render detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2), \n",
    "                                mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2) \n",
    "                                 )               \n",
    "        \n",
    "        # Display and write the video\n",
    "        if ret == True:\n",
    "            cv2.imshow('Mediapipe Feed', image)\n",
    "            output.write(image)\n",
    "\n",
    "            # Pause or stop the video when instructed\n",
    "            key = cv2.waitKey(5)\n",
    "            # Stop by pressing 'q'\n",
    "            if key == ord('q'):\n",
    "                break\n",
    "            # Pause by pressing 'w', resume by pressing any other key\n",
    "            if key == ord('w'):\n",
    "                cv2.waitKey(-1)\n",
    "\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
