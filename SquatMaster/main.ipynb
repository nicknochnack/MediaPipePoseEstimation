{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d91400d",
   "metadata": {},
   "source": [
    "# Squat Tutorial\n",
    "\n",
    "## Mission\n",
    "\n",
    "We are developing a code base that can guide the user in performing squats with good form, correct his limb positioning, and counting down his reps.\n",
    "\n",
    "### Step 1 - Correct body positioning\n",
    "\n",
    "Guidance by showing the user on the screen\n",
    "\n",
    "1. Optimal positioning of the legs\n",
    "    Feet - Shoulder width\n",
    "    ### Too narrow\n",
    "    ![Feetwidth-wrong.jpeg](attachment:Feetwidth-wrong.jpeg)\n",
    "\n",
    "    ### Correct\n",
    "    ![Feetwidth-correct.jpeg](attachment:Feetwidth-correct.jpeg)\n",
    "    \n",
    "    \n",
    "    Feet angle - 20-30 deg\n",
    "    ### Too straight \n",
    "    ![Feet angle - not correct.jpeg](<attachment:Feet angle - not correct.jpeg>)\n",
    "\n",
    "    ### Correct\n",
    "    ![Feet angle - correct.jpeg](<attachment:Feet angle - correct.jpeg>)\n",
    "    \n",
    "\n",
    "2. neutral position of the hips\n",
    "### Rotated\n",
    "![Rotated hip.jpeg](<attachment:Rotated hip.jpeg>)\n",
    "\n",
    "### Neutral hips \n",
    "![Neutral hip position.jpeg](<attachment:Neutral hip position.jpeg>)\n",
    "\n",
    "\n",
    "3. Correct pretension on the knees \n",
    "4. Bracing the core \n",
    "\n",
    "[Video Guidance](https://www.youtube.com/watch?v=gcNh17Ckjgg&ab_channel=JeremyEthier)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The guidance will be provided by markers on the recording screen window \n",
    "\n",
    "### Step 2 - Ready to start the set\n",
    "\n",
    "When the user is ready, he/she will give a thumbs up to indicate the same. We will use thumbs symbol as a placeholder\n",
    "\n",
    "\n",
    "### Step 3 - Form monitoring, counting the rep\n",
    "\n",
    "The code will calculate and signal if the correct depth is reached by providing a green tick and reduce one from the target repetitions.\n",
    "\n",
    "#### What are the most forms of mistakes the user can do?\n",
    "Here is a summary of the video you requested:\n",
    "\n",
    "- **How to squat properly**: The video explains the common mistakes and the correct technique of performing a barbell squat, which is a compound exercise that works the quads, glutes, hamstrings, and core muscles.\n",
    "- **Common mistakes**: The video identifies four common mistakes that people make when squatting: 1) Not bracing the core properly, which can lead to lower back pain and injury. 2) Not breaking at the hips first, which can cause the knees to cave in and the heels to lift off the ground. 3) Not going deep enough, which can limit the muscle activation and the range of motion. 4) Not keeping the elbows down, which can create tension in the upper back and shoulders and affect the bar path.\n",
    "- **Correct technique**: The video demonstrates the correct technique of squatting with the following steps: 1) Set up the bar on the rack at shoulder height and grip it with a comfortable width. 2) Step under the bar and position it on the upper back, either on the traps (high bar) or the rear delts (low bar). 3) Unrack the bar and take a few steps back. 4) Set your feet slightly wider than shoulder width and point your toes slightly outwards. 5) Brace your core by taking a deep breath and squeezing your abs. 6) Break at the hips first and then bend your knees to lower yourself until your thighs are at least parallel to the ground. 7) Keep your chest up, your back straight, your elbows down, and your knees in line with your toes. 8) Drive through your heels and push your hips forward to stand up. 9) Exhale at the top and repeat for the desired number of reps.\n",
    "- **Tips and variations**: The video also provides some tips and variations to improve your squat performance and avoid injuries. Some of them are: 1) Warm up properly before squatting with some mobility exercises and lighter sets. 2) Use a belt to increase your intra-abdominal pressure and stability. 3) Use a spotter or safety pins to help you in case of failure. 4) Vary your stance width and bar position to target different muscle groups and find your optimal form. 5) Incorporate some accessory exercises, such as lunges, leg presses, and leg curls, to strengthen your weak points and prevent imbalances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e3013c",
   "metadata": {},
   "source": [
    "1. Installing libraries\n",
    "\n",
    "This line installs the required packages for the tutorial, specifically `mediapipe` (which is a library for hand, pose, and face detection) and `opencv-python` (a library for computer vision tasks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a84f9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mediapipe opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f0884c",
   "metadata": {},
   "source": [
    "2. Installing Dependencies \n",
    "\n",
    "Here, the necessary libraries are imported:\n",
    "\n",
    "- `cv2`: This is the OpenCV library which is used for various computer vision tasks.\n",
    "- `mediapipe as mp`: Mediapipe is imported with the alias `mp`.\n",
    "- `numpy as np`: Numpy is imported with the alias `np`.\n",
    "- `mp_drawing`: This is a utility from Mediapipe to help with drawing on images.\n",
    "- `mp_pose`: This helps in detecting human poses using Mediapipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ba69a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06df7edb",
   "metadata": {},
   "source": [
    "3. This block of code sets up a video feed using OpenCV.\n",
    "\n",
    "- `cv2.VideoCapture(0)`: This initializes the video capture with the primary webcam of the computer.\n",
    "- `while cap.isOpened()`: Continuously capture frames as long as the webcam is active.\n",
    "- `ret, frame = cap.read()`: Read a frame from the webcam. `ret` is a boolean that indicates if the frame was successfully grabbed.\n",
    "- `cv2.imshow('Mediapipe Feed', frame)`: This displays the captured frame in a window named 'Mediapipe Feed'.\n",
    "- `if cv2.waitKey(10) & 0xFF == ord('q')`: This checks if the 'q' key is pressed. If pressed, it breaks out of the loop to stop capturing.\n",
    "- `cap.release()`: Releases the video capture object.\n",
    "- `cv2.destroyAllWindows()`: Closes all OpenCV windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbcf1691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIDEO FEED\n",
    "cap = cv2.VideoCapture(1)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab a frame\")\n",
    "        break\n",
    "    cv2.imshow('Mediapipe Feed', frame)\n",
    "\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a277cf",
   "metadata": {},
   "source": [
    "4. Pose detection\n",
    "\n",
    "This block of code captures video from the webcam and processes each frame using Mediapipe to detect human poses.\n",
    "The code inside the `with` statement initializes the pose detection from Mediapipe.\n",
    "Each frame from the webcam is converted from BGR (Blue, Green, Red - the default format of OpenCV) to RGB (Red, Green, Blue - the format Mediapipe uses).\n",
    "`pose.process(image)`: Detects the pose in the image.\n",
    "The detected landmarks (keypoints of the pose) are then drawn on the image using `mp_drawing.draw_landmarks(...)`. The processed image is then displayed using `cv2.imshow(...)`. The loop continues until the 'q' key is pressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4798595e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.1) /Users/xperience/GHA-OpenCV-Python/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m/Users/vijaishankarbhavanishankar/Documents/EschAR/MediaPipePoseEstimation-eschar/SquatMaster/main.ipynb Cell 9\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vijaishankarbhavanishankar/Documents/EschAR/MediaPipePoseEstimation-eschar/SquatMaster/main.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m ret, frame \u001b[39m=\u001b[39m cap\u001b[39m.\u001b[39mread()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vijaishankarbhavanishankar/Documents/EschAR/MediaPipePoseEstimation-eschar/SquatMaster/main.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Recolor image to RGB\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/vijaishankarbhavanishankar/Documents/EschAR/MediaPipePoseEstimation-eschar/SquatMaster/main.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(frame, cv2\u001b[39m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vijaishankarbhavanishankar/Documents/EschAR/MediaPipePoseEstimation-eschar/SquatMaster/main.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m image\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mwriteable \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vijaishankarbhavanishankar/Documents/EschAR/MediaPipePoseEstimation-eschar/SquatMaster/main.ipynb#X10sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Make detection\u001b[39;00m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.8.1) /Users/xperience/GHA-OpenCV-Python/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "## Setup mediapipe instance\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # Recolor image to RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "      \n",
    "        # Make detection\n",
    "        results = pose.process(image)\n",
    "    \n",
    "        # Recolor back to BGR\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Render detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2), \n",
    "                                mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2) \n",
    "                                 )               \n",
    "        \n",
    "        cv2.imshow('Mediapipe Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b11bb91",
   "metadata": {},
   "source": [
    "This line is likely an interactive command to inspect the documentation or source code of the `DrawingSpec` class from Mediapipe. In Jupyter notebooks, appending `??` to a function or class name displays its source code and docstring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04fc6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing.DrawingSpec??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5899c6a",
   "metadata": {},
   "source": [
    "This block of code is very similar to the one we discussed earlier. The main addition is the extraction of landmarks.\n",
    "The line `landmarks = results.pose_landmarks.landmark` extracts the pose landmarks (keypoints) detected by Mediapipe. These landmarks represent different parts of the body, like the nose, eyes, shoulders, elbows, etc.\n",
    "The extracted landmarks are then printed to the console.\n",
    "The `try` and `except` block is used to handle any potential errors that might arise when extracting landmarks. If there's an error (e.g., no landmarks detected), the code inside the `except` block is executed, which in this case, does nothing (using `pass`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e396dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "## Setup mediapipe instance\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # Recolor image to RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "      \n",
    "        # Make detection\n",
    "        results = pose.process(image)\n",
    "    \n",
    "        # Recolor back to BGR\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Extract landmarks\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            print(landmarks)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        # Render detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2), \n",
    "                                mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2) \n",
    "                                 )               \n",
    "        \n",
    "        cv2.imshow('Mediapipe Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7188ab8b",
   "metadata": {},
   "source": [
    "This line simply gets and displays the number of landmarks detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563b7fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(landmarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b77f08d",
   "metadata": {},
   "source": [
    "This block of code prints out all the pose landmarks that Mediapipe can detect. `mp_pose.PoseLandmark` is an enumeration of all possible pose landmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025e7cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lndmrk in mp_pose.PoseLandmark:\n",
    "    print(lndmrk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6865dd9e",
   "metadata": {},
   "source": [
    "This line gets the visibility score of the left shoulder landmark. The visibility score indicates the probability of the landmark being visible in the image (not occluded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0b6d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].visibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cfa281",
   "metadata": {},
   "source": [
    "This line retrieves details about the left elbow landmark, such as its coordinates and visibility score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8fb229",
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a617e626",
   "metadata": {},
   "source": [
    "This line retrieves the details of the left wrist landmark, such as its coordinates (x, y, and z) and potentially other details like visibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8582414",
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19970f40",
   "metadata": {},
   "source": [
    "\n",
    "This function, named `calculate_angle`, is designed to compute the angle formed by three points: `a`, `b`, and `c`, where `b` acts as the vertex of the angle.\n",
    "The function performs the following steps:\n",
    "- Converts the points `a`, `b`, and `c` to numpy arrays to facilitate mathematical operations.\n",
    "- Calculates the angle using the arctangent function. The difference in the arctangents of the slopes defined by the points gives the angle in radians.\n",
    "- Converts the angle from radians to degrees.\n",
    "- If the resulting angle exceeds 180 degrees, it calculates the supplementary angle.\n",
    "- Returns the computed angle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc05493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angle(a,b,c):\n",
    "    a = np.array(a) # First\n",
    "    b = np.array(b) # Mid\n",
    "    c = np.array(c) # End\n",
    "    \n",
    "    radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])\n",
    "    angle = np.abs(radians*180.0/np.pi)\n",
    "    \n",
    "    if angle >180.0:\n",
    "        angle = 360-angle\n",
    "        \n",
    "    return angle "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d24750",
   "metadata": {},
   "source": [
    "\n",
    "This block captures video from the webcam and processes each frame with Mediapipe to detect human poses.\n",
    "The major steps involved are:\n",
    "1. **Frame Capture**: The video capture is initialized, and frames are continuously read.\n",
    "2. **Color Conversion**: Each frame is converted from BGR to RGB, which is the format expected by Mediapipe.\n",
    "3. **Pose Detection**: The frame is processed using Mediapipe's pose detection. The result contains pose landmarks.\n",
    "4. **Angle Calculation**:\n",
    "   - The landmarks corresponding to the left shoulder, elbow, and wrist are extracted.\n",
    "   - The angle at the elbow (formed by the shoulder, elbow, and wrist) is calculated using the `calculate_angle` function.\n",
    "   - This angle is displayed on the video feed near the elbow position.\n",
    "5. **Bicep Curl Detection**: A simple logic is implemented to detect the motion of a bicep curl exercise:\n",
    "   - When the arm is fully extended (angle > 160 degrees), the stage is set to \"down\".\n",
    "   - If the arm is curled (angle < 30 degrees) and the previous stage was \"down\", the counter increments, indicating a completed bicep curl repetition.\n",
    "6. **Display Information**: The counter and the stage (\"up\" or \"down\") are displayed on the video feed.\n",
    "7. **Landmark Rendering**: Detected landmarks and their connections are drawn on the frame.\n",
    "8. **Display Video Feed**: The processed frame, with landmarks and angle information, is displayed in a window.\n",
    "9. **Exit Condition**: The video feed can be stopped by pressing the 'q' key.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e7ff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(1)\n",
    "## Setup mediapipe instance\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # Recolor image to RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "      \n",
    "        # Make detection\n",
    "        results = pose.process(image)\n",
    "    \n",
    "        # Recolor back to BGR\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Extract landmarks\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            \n",
    "            # Get coordinates\n",
    "            shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x,landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]\n",
    "            elbow = [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x,landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]\n",
    "            wrist = [landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].x,landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].y]\n",
    "            \n",
    "            # Calculate angle\n",
    "            angle = calculate_angle(shoulder, elbow, wrist)\n",
    "            \n",
    "            # Visualize angle\n",
    "            cv2.putText(image, str(angle), \n",
    "                           tuple(np.multiply(elbow, [640, 480]).astype(int)), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA\n",
    "                                )\n",
    "                       \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        # Render detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2), \n",
    "                                mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2) \n",
    "                                 )               \n",
    "        \n",
    "        cv2.imshow('Mediapipe Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb98993e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "# Curl counter variables\n",
    "counter = 0 \n",
    "stage = None\n",
    "\n",
    "## Setup mediapipe instance\n",
    "with mp_pose.Pose(min_detection_confidence=0.1, min_tracking_confidence=0.1) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # Recolor image to RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "      \n",
    "        # Make detection\n",
    "        results = pose.process(image)\n",
    "    \n",
    "        # Recolor back to BGR\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Extract landmarks\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            \n",
    "            # Get coordinates\n",
    "            shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x,landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]\n",
    "            elbow = [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x,landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]\n",
    "            wrist = [landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].x,landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].y]\n",
    "            \n",
    "            # Calculate angle\n",
    "            angle = calculate_angle(shoulder, elbow, wrist)\n",
    "            \n",
    "            # Visualize angle\n",
    "            cv2.putText(image, str(angle), \n",
    "                           tuple(np.multiply(elbow, [640, 480]).astype(int)), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA\n",
    "                                )\n",
    "            \n",
    "            # Curl counter logic\n",
    "            if angle > 160:\n",
    "                stage = \"down\"\n",
    "            if angle < 30 and stage =='down':\n",
    "                stage=\"up\"\n",
    "                counter +=1\n",
    "                print(counter)\n",
    "                       \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Render curl counter\n",
    "        # Setup status box\n",
    "        cv2.rectangle(image, (0,0), (225,73), (245,117,16), -1)\n",
    "        \n",
    "        # Rep data\n",
    "        cv2.putText(image, 'REPS', (15,12), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 1, cv2.LINE_AA)\n",
    "        cv2.putText(image, str(counter), \n",
    "                    (10,60), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 2, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Stage data\n",
    "        cv2.putText(image, 'STAGE', (65,12), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 1, cv2.LINE_AA)\n",
    "        cv2.putText(image, stage, \n",
    "                    (60,60), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 2, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        \n",
    "        # Render detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2), \n",
    "                                mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2) \n",
    "                                 )               \n",
    "        \n",
    "        cv2.imshow('Mediapipe Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db12dfe",
   "metadata": {},
   "source": [
    "tasks for squat tutorial\n",
    "\n",
    "1. leg and hand positions before start of exercise\n",
    "2. signal that the exercise person ready\n",
    "3. countdown to exercise \n",
    "4. counting reps\n",
    "5. countingn time - up and down\n",
    "6. create a mock up of a gym camera so we have the same reference points \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928524af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# Define a function that calculates an angle between three points in x,y space\n",
    "def calculate_angle(a,b,c):\n",
    "    a = np.array(a) # First\n",
    "    b = np.array(b) # Mid\n",
    "    c = np.array(c) # End\n",
    "    \n",
    "    radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])\n",
    "    angle = np.abs(radians*180.0/np.pi)\n",
    "    \n",
    "    if angle >180.0:\n",
    "        angle = 360-angle\n",
    "    return angle \n",
    "\n",
    "# Specify the location of the file with the video to be read\n",
    "filename_r = '/Users/vijaishankarbhavanishankar/Documents/EschAR/MediaPipePoseEstimation-eschar/Data/videos/IMG_3620_RepCountV2.mp4'\n",
    "# Specify the location where the new video with detections will be written\n",
    "filename_w = '/Users/vijaishankarbhavanishankar/Documents/EschAR/MediaPipePoseEstimation-eschar/Data/videos/IMG_3620_RepCountV2-output.mp4'\n",
    "# Capture the video from the file\n",
    "cap = cv2.VideoCapture(filename_r)\n",
    "\n",
    "# Get video frame dimensions and fps\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "frame_size = (frame_width,frame_height)\n",
    "fps = cap.get(5)\n",
    "\n",
    "# Create a video writer object\n",
    "output = cv2.VideoWriter(filename_w, cv2.VideoWriter_fourcc(*'XVID'), fps, frame_size)\n",
    "\n",
    "# Initialize the Rep Counter variables\n",
    "real_counter = 0\n",
    "stage = None\n",
    "rep_count = 0\n",
    "last_event_time = None\n",
    "frame_count = 0\n",
    "\n",
    "## Setup mediapipe instance\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "\n",
    "    fps = cap.get(5)\n",
    "    frame_width  = int(cap.get(3))\n",
    "    frame_height = int(cap.get(4))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to grab a frame\")\n",
    "            break\n",
    "        # Count frames and calculate time in video\n",
    "        frame_count += 1\n",
    "        time = frame_count / fps #[s]\n",
    "\n",
    "        # Recolor image to RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "      \n",
    "        # Make detection\n",
    "        results = pose.process(image)\n",
    "    \n",
    "        # Recolor back to BGR\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Extract landmarks\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            \n",
    "            # Get coordinates\n",
    "            hip = [landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].x,landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].y]\n",
    "            knee = [landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].x,landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].y]\n",
    "            ankle = [landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].x,landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].y]\n",
    "            \n",
    "            # Calculate angle\n",
    "            angle = calculate_angle(hip, knee, ankle)\n",
    "            \n",
    "            # Visualize angle\n",
    "            cv2.putText(image, f\"{angle:.1f}\", \n",
    "                           tuple(np.multiply(knee, frame_size).astype(int)), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA\n",
    "                                )\n",
    "            \n",
    "            ##########################################################################################################################\n",
    "            # Rep Counter logic\n",
    "            # The rep is registered when the knee bend angle goes from the upper limit to the lower limit\n",
    "            upper_limit = 160\n",
    "            lower_limit = 80\n",
    "            # Specify the minimum rep number to start registering the set\n",
    "            min_rep_count = 2\n",
    "            # Specify the minimum time between reps\n",
    "            min_rep_time = 3 #[s]\n",
    "            # Above the upper angle limit, register the \"hold up\" stage\n",
    "            if angle >= upper_limit:\n",
    "                stage = 'hold up'\n",
    "            # Below the upper limit and after the \"hold up\" stage, register \"down\" stage\n",
    "            if angle < upper_limit and stage == 'hold up':\n",
    "                stage = 'down'\n",
    "            # Below the lower limit and after the \"down\" stage, register the \"hold down\" stage\n",
    "            if angle < lower_limit and stage =='down':\n",
    "                stage = 'hold down'\n",
    "            # Above the lower limit and after the \"hold down\" stage, register the \"up\" stage and count the rep\n",
    "            if angle > lower_limit and stage == 'hold down':\n",
    "                stage = 'up'\n",
    "                rep_count +=1\n",
    "                last_event_time = time\n",
    "                print(last_event_time)\n",
    "\n",
    "            # Register the real rep count only for sets with more than the min rep count\n",
    "            if rep_count >= min_rep_count:\n",
    "                real_counter = rep_count\n",
    "            # Restart the counter when the reps do not repeat in less than the min rep time\n",
    "            if time - last_event_time > min_rep_time:\n",
    "                rep_count = 0\n",
    "            ##########################################################################################################################\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        # Render Rep Counter\n",
    "        # Set the position of the Rep Counter window\n",
    "        x_offset = 50\n",
    "        y_offset = 100\n",
    "        # Setup status box\n",
    "        cv2.rectangle(image, (x_offset,y_offset), (390+x_offset,73+y_offset), (128,128,128), -1)\n",
    "        # Rep data\n",
    "        cv2.putText(image, 'REPS', (15+x_offset,12+y_offset), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 1, cv2.LINE_AA)\n",
    "        cv2.putText(image, str(real_counter), \n",
    "                    (10+x_offset,60+y_offset), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 2, (255,255,255), 2, cv2.LINE_AA)\n",
    "        # Stage data\n",
    "        cv2.putText(image, 'STAGE', (65+x_offset,12+y_offset), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 1, cv2.LINE_AA)\n",
    "        cv2.putText(image, stage, \n",
    "                    (60+x_offset,60+y_offset), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 2, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "\n",
    "        # Render detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2), \n",
    "                                mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2) \n",
    "                                 )               \n",
    "        \n",
    "        # Display and write the video\n",
    "        if ret == True:\n",
    "            cv2.imshow('Mediapipe Feed', image)\n",
    "            output.write(image)\n",
    "\n",
    "            # Pause or stop the video when instructed\n",
    "            key = cv2.waitKey(5)\n",
    "            # Stop by pressing 'q'\n",
    "            if key == ord('q'):\n",
    "                break\n",
    "            # Pause by pressing 'w', resume by pressing any other key\n",
    "            if key == ord('w'):\n",
    "                cv2.waitKey(-1)\n",
    "\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
